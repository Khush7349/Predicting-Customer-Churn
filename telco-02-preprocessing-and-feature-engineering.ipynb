{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-18T02:47:24.015965Z",
     "iopub.status.busy": "2025-09-18T02:47:24.015067Z",
     "iopub.status.idle": "2025-09-18T02:47:24.362963Z",
     "shell.execute_reply": "2025-09-18T02:47:24.362086Z",
     "shell.execute_reply.started": "2025-09-18T02:47:24.015930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T02:49:21.199312Z",
     "iopub.status.busy": "2025-09-18T02:49:21.198657Z",
     "iopub.status.idle": "2025-09-18T02:49:21.241240Z",
     "shell.execute_reply": "2025-09-18T02:49:21.240402Z",
     "shell.execute_reply.started": "2025-09-18T02:49:21.199278Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification: Missing values in 'TotalCharges' after cleaning:\n",
      "0\n",
      "\n",
      "Data type of 'TotalCharges' after cleaning:\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "# --- Clean the TotalCharges column ---\n",
    "file_path = '/kaggle/input/telco-customer-churn/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
    "print(\"Verification: Missing values in 'TotalCharges' after cleaning:\")\n",
    "print(df['TotalCharges'].isnull().sum())\n",
    "print(\"\\nData type of 'TotalCharges' after cleaning:\")\n",
    "print(df['TotalCharges'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T02:50:59.286236Z",
     "iopub.status.busy": "2025-09-18T02:50:59.285391Z",
     "iopub.status.idle": "2025-09-18T02:51:00.031021Z",
     "shell.execute_reply": "2025-09-18T02:51:00.030152Z",
     "shell.execute_reply.started": "2025-09-18T02:50:59.286206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Data Split Complete ##\n",
      "Training set shape: (5634, 19)\n",
      "Testing set shape: (1409, 19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['customerID', 'Churn'], axis=1)\n",
    "y = df['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(\"## Data Split Complete ##\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T02:52:55.835537Z",
     "iopub.status.busy": "2025-09-18T02:52:55.834970Z",
     "iopub.status.idle": "2025-09-18T02:52:55.844200Z",
     "shell.execute_reply": "2025-09-18T02:52:55.843341Z",
     "shell.execute_reply.started": "2025-09-18T02:52:55.835506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Features: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "Categorical Features: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n"
     ]
    }
   ],
   "source": [
    "categorical_features = X_train.select_dtypes(include=['object']).columns\n",
    "numerical_features = X_train.select_dtypes(include=np.number).columns\n",
    "print(f\"Numerical Features: {list(numerical_features)}\")\n",
    "print(f\"Categorical Features: {list(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T02:54:38.812387Z",
     "iopub.status.busy": "2025-09-18T02:54:38.811959Z",
     "iopub.status.idle": "2025-09-18T02:54:38.928758Z",
     "shell.execute_reply": "2025-09-18T02:54:38.928007Z",
     "shell.execute_reply.started": "2025-09-18T02:54:38.812359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Preprocessing Complete ##\n",
      "Shape of processed training data: (5634, 45)\n",
      "Shape of processed testing data: (1409, 45)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(\"\\n## Preprocessing Complete ##\")\n",
    "print(f\"Shape of processed training data: {X_train_processed.shape}\")\n",
    "print(f\"Shape of processed testing data: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T02:58:46.639194Z",
     "iopub.status.busy": "2025-09-18T02:58:46.638455Z",
     "iopub.status.idle": "2025-09-18T02:58:46.660403Z",
     "shell.execute_reply": "2025-09-18T02:58:46.659559Z",
     "shell.execute_reply.started": "2025-09-18T02:58:46.639164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Notebook 2 Complete ##\n",
      "All processed data and the preprocessor have been saved to the output directory.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "np.save('X_train_processed.npy', X_train_processed)\n",
    "np.save('X_test_processed.npy', X_test_processed)\n",
    "y_train.to_csv('y_train.csv', index=False, header=True)\n",
    "y_test.to_csv('y_test.csv', index=False, header=True)\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "print(\"## Notebook 2 Complete ##\")\n",
    "print(\"All processed data and the preprocessor have been saved to the output directory.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 13996,
     "sourceId": 18858,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
